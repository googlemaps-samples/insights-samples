{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a189c3d8",
   "metadata": {
    "id": "e6f2f438"
   },
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733b62e5",
   "metadata": {
    "id": "61d1bfb1"
   },
   "source": [
    "# Identify lamp make and model using Gemini 2.5 flash and a RAG\n",
    "\n",
    " This notebook demonstrates how to identify the make and model of a lamp using the Gemini 2.5 Flash model with a Retrieval Augmented Generation (RAG) approach.\n",
    "\n",
    " **Notebook Description:**\n",
    "\n",
    " The primary goal of this notebook is to leverage the advanced multimodal capabilities of Gemini 2.5 Flash to analyze images of lamps and extract specific information about their make and model. It employs a RAG strategy, which means it will likely involve:\n",
    "\n",
    " 1.  **Image Input:** You'll provide images of lamps as input.\n",
    " 2.  **Information Retrieval (RAG):** The notebook will likely access an external knowledge base or a set of documents containing information about various lamp makes and models. This could be a local dataset, a cloud-hosted database, or even web search results.\n",
    " 3.  **Gemini 2.5 Flash Integration:** The Gemini 2.5 Flash model will process the input image and the retrieved information to identify the most probable make and model of the lamp.\n",
    " 4.  **Output:** The notebook will output the identified make and model, potentially with a confidence score or additional descriptive details.\n",
    "\n",
    " **Prerequisites for First-Time Users:**\n",
    "\n",
    " To successfully run this notebook, please ensure the following:\n",
    "\n",
    " 1.  **Google Cloud Project:** You need an active Google Cloud project.\n",
    " 2.  **Enable APIs:**\n",
    "     *   **Vertex AI API:** This is essential for accessing and using Gemini models. You can enable it through the Google Cloud Console under \"APIs & Services\" > \"Library\".\n",
    " 3.  **Authentication:**\n",
    "     *   **Google Cloud Authentication:** Ensure your Colab environment is authenticated to your Google Cloud project. You can do this by running the following command in a code cell:\n",
    "         ```python\n",
    "         from google.colab import auth\n",
    "         auth.authenticate_user()\n",
    "         ```\n",
    "         Follow the prompts to log in with your Google account that has access to the project.\n",
    "     *   **Service Account (Optional but Recommended for Production):** For more robust applications, consider setting up a service account with appropriate permissions (e.g., Vertex AI User role) and downloading its JSON key file. You can then authenticate using:\n",
    "       ```python\n",
    "         from google.colab import auth\n",
    "         from google.oauth2 import service_account\n",
    "\n",
    "         # Replace 'path/to/your/service_account.json' with the actual path to your key file\n",
    "         credentials = service_account.Credentials.from_service_account_file('path/to/your/service_account.json')\n",
    "         auth.authenticate_user(credentials=credentials)\n",
    "         ```\n",
    " 4.  **Gemini API Key (if not using Vertex AI):** While this notebook likely uses Vertex AI, if it were to use a direct Gemini API, you would need to obtain an API key from Google AI Studio or Google Cloud Console.\n",
    " 5.  **Input Data:**\n",
    "     *   **Lamp Images:** Prepare the images of the lamps you want to identify. These should be uploaded to your Colab environment or accessible from Google Cloud Storage.\n",
    "     *   **RAG Data Source:** If the RAG component relies on a specific data source (e.g., a CSV file, a database, or a collection of documents), ensure this data is accessible and in the correct format. This might involve uploading files to Colab or configuring access to cloud storage.\n",
    " 6.  **Required Libraries:** The notebook will likely import several libraries. Ensure they are installed. Common ones include:\n",
    "     ```python\n",
    "     !pip install google-cloud-aiplatform google-cloud-storage Pillow\n",
    "     ```\n",
    "     (The specific libraries might vary based on the notebook's implementation.)\n",
    "\n",
    " By following these steps, you should be well-equipped to run this notebook and explore the capabilities of Gemini 2.5 Flash for lamp identification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e221b3e8",
   "metadata": {
    "id": "ab074d90"
   },
   "source": [
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73db7074",
   "metadata": {
    "id": "23f28877"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade google-cloud-bigquery google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b155ba0",
   "metadata": {
    "id": "de710bd6"
   },
   "source": [
    "## Configuration\n",
    "\n",
    "**Important**: Replace the placeholder values below with your actual GCP Project ID and Region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123e099e",
   "metadata": {
    "id": "eefe0d49"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = ''  # @param {type:\"string\"}\n",
    "REGION = 'us-central1'      # @param {type:\"string\"}\n",
    "BIGQUERY_DATASET = 'imagery_insights___preview___us' # @param {type:\"string\"}\n",
    "BIGQUERY_TABLE = 'latest_observations' # @param {type:\"string\"}\n",
    "ASSET_TYPE = 'ASSET_CLASS_UTILITY_POLE' # @param {type:\"string\"}\n",
    "LIMIT = 100 # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cf87b3",
   "metadata": {
    "id": "70c48cc9"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import vertexai\n",
    "from google.cloud import bigquery\n",
    "from google import genai\n",
    "from google.genai.types import Content, Part, Tool, Retrieval, VertexRagStore, VertexRagStoreRagResource\n",
    "\n",
    "# Initialize Vertex AI\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "print(\"Libraries imported and Vertex AI initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bdd90b",
   "metadata": {
    "id": "ce2a0696"
   },
   "source": [
    "## Fetch Image URIs from BigQuery\n",
    "\n",
    "Next, we'll query a BigQuery table to get the GCS URIs of the images we want to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bfee4e",
   "metadata": {
    "id": "df6a38c6"
   },
   "outputs": [],
   "source": [
    "# Updated SQL to explicitly select the required columns\n",
    "BIGQUERY_SQL_QUERY = f\"\"\"\n",
    "SELECT\n",
    "  asset_id,\n",
    "  observation_id,\n",
    "  gcs_uri\n",
    "FROM\n",
    "  `{PROJECT_ID}.{BIGQUERY_DATASET}.{BIGQUERY_TABLE}`\n",
    "WHERE\n",
    "  asset_type = \"{ASSET_TYPE}\"\n",
    "  AND gcs_uri IS NOT NULL\n",
    "LIMIT {LIMIT};\n",
    "\"\"\"\n",
    "\n",
    "# Execute BigQuery Query and store the full row data\n",
    "try:\n",
    "    bigquery_client = bigquery.Client(project=PROJECT_ID)\n",
    "    query_job = bigquery_client.query(BIGQUERY_SQL_QUERY)\n",
    "    # This new variable will hold all the data needed for the DataFrame\n",
    "    image_data_from_bq = [dict(row) for row in query_job]\n",
    "\n",
    "    # For compatibility with the existing loop, we also extract the URIs\n",
    "    gcs_uris = [item.get(\"gcs_uri\") for item in image_data_from_bq if item.get(\"gcs_uri\")]\n",
    "\n",
    "    print(f\"Successfully fetched {len(image_data_from_bq)} items from BigQuery:\")\n",
    "    for item in image_data_from_bq:\n",
    "        print(f\"  - {item}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while querying BigQuery: {e}\")\n",
    "    image_data_from_bq = []\n",
    "    gcs_uris = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6843ad",
   "metadata": {
    "id": "b05f2cff"
   },
   "source": [
    "## Define Image Classification Function\n",
    "\n",
    "This function takes a GCS URI and a prompt, then uses the Gemini 2.5 Flash model to generate a description of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a61c2c",
   "metadata": {
    "id": "126856a8"
   },
   "outputs": [],
   "source": [
    "def classify_image_with_gemini(gcs_uri: str, prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Classifies an image using the Gemini 2.5 Flash model by directly passing its GCS URI.\n",
    "    \"\"\"\n",
    "    MODEL = \"gemini-2.5-flash\" # @param {type:\"string\"}\n",
    "\n",
    "    try:\n",
    "        model = genai.GenerativeModel(MODEL)\n",
    "        image_part = Part.from_uri(uri=gcs_uri, mime_type=\"image/jpeg\")\n",
    "        responses = model.generate_content([image_part, prompt])\n",
    "        return responses.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error classifying image from URI {gcs_uri}: {e}\")\n",
    "        return \"Classification failed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473d8a74",
   "metadata": {
    "id": "1ae4daad"
   },
   "source": [
    "## Setup for RAG\n",
    "\n",
    "### Subtask:\n",
    "Import the necessary libraries for Vertex AI Search and configure the RAG datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c1ddd1",
   "metadata": {
    "id": "85b3e0de"
   },
   "outputs": [],
   "source": [
    "# Define the path to the RAG Corpus\n",
    "RAG_CORPUS_PATH = \"projects/sarthaks-lab/locations/us-east4/ragCorpora/6838716034162098176\" # @param {type:\"string\"}\n",
    "\n",
    "print(\"Configuration and imports updated for the new RAG method.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6efc05",
   "metadata": {
    "id": "a3b4c5d6"
   },
   "source": [
    "## Create a Lamp Post Check Function\n",
    "\n",
    "### Subtask:\n",
    "Develop a function that takes an image URI and uses a simple prompt to quickly determine if the image contains a lamp post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e4ac89",
   "metadata": {
    "id": "e7f8g9h0"
   },
   "outputs": [],
   "source": [
    "def is_lamp_post(gcs_uri: str) -> bool:\n",
    "    \"\"\"\n",
    "    Uses the Gemini model to quickly check if an image contains a lamp post.\n",
    "    \"\"\"\n",
    "    MODEL = \"gemini-2.5-flash\" # @param {type:\"string\"}\n",
    "\n",
    "    try:\n",
    "        model = genai.GenerativeModel(MODEL)\n",
    "        image_part = Part.from_uri(uri=gcs_uri, mime_type=\"image/jpeg\")\n",
    "        prompt = \"Does this image contain a lamp post or a street light? Answer with only 'yes' or 'no'.\"\n",
    "\n",
    "        responses = model.generate_content([image_part, prompt])\n",
    "\n",
    "        # Clean up the response and check for a 'yes'\n",
    "        return responses.text.strip().lower() == 'yes'\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking image {gcs_uri}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage with a placeholder URI (will not work without a valid URI)\n",
    "# print(is_lamp_post(\"gs://your-bucket/path/to/image.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12267ef0",
   "metadata": {
    "id": "i1j2k3l4"
   },
   "source": [
    "## Create a Detailed Description Function\n",
    "\n",
    "This cell defines a Python function `is_lamp_post` that takes a Google Cloud Storage (GCS) URI of an image as input. It utilizes the `gemini-2.5-flash` model from Vertex AI to determine if the image contains a lamp post or street light. The function sends the image and a specific prompt to the model and returns `True` if the model's response, after cleaning and converting to lowercase, is exactly 'yes', and `False` otherwise. It also includes basic error handling for the model interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74993f4",
   "metadata": {
    "id": "m5n6o7p8"
   },
   "outputs": [],
   "source": [
    "def get_detailed_description(gcs_uri: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses the Gemini model to generate a detailed description of a lamp post from an image.\n",
    "    \"\"\"\n",
    "    MODEL = \"gemini-2.5-flash\" # @param {type:\"string\"}\n",
    "\n",
    "    try:\n",
    "        model = genai.GenerativeModel(MODEL)\n",
    "        image_part = Part.from_uri(uri=gcs_uri, mime_type=\"image/jpeg\")\n",
    "        prompt = \"\"\"\n",
    "        Describe the lamp post in this image in detail. Focus on the following features:\n",
    "        - **Overall style**: (e.g., modern, vintage, ornate, simple)\n",
    "        - **Pole material and color**: (e.g., black metal, grey concrete, brown wood)\n",
    "        - **Lamp head shape and design**: (e.g., lantern-style, cobra head, globe, multi-light fixture)\n",
    "        - **Light source**: (e.g., LED, bulb type if visible)\n",
    "        - **Any distinctive features**: (e.g., decorative elements, banners, bases, arms)\n",
    "        Provide a concise, structured summary of these features.\n",
    "        \"\"\"\n",
    "\n",
    "        responses = model.generate_content([image_part, prompt])\n",
    "        return responses.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating description for {gcs_uri}: {e}\")\n",
    "        return \"Description generation failed.\"\n",
    "\n",
    "# Example usage with a placeholder URI (will not work without a valid URI)\n",
    "# print(get_detailed_description(\"gs://your-bucket/path/to/image.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9r1s2t3"
   },
   "source": [
    "## Image Classification and Description with Gemini\n",
    "\n",
    "This section of the notebook focuses on leveraging the Gemini 2.5 Flash model for image analysis. It defines two key functions:\n",
    "\n",
    "1.  `is_lamp_post`: This function quickly determines if an image contains a lamp post or street light by sending the image and a specific \"yes/no\" prompt to the Gemini model.\n",
    "2.  `get_detailed_description`: This function provides a more in-depth analysis, generating a structured and detailed description of a lamp post within an image, focusing on specific features like style, material, color, and design.\n",
    "\n",
    "These functions are designed to process images from GCS URIs and extract relevant information using advanced AI capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4v5w6x7"
   },
   "outputs": [],
   "source": [
    "def get_make_and_model(client: genai.Client, description: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses the RAG-grounded Gemini model to identify the make and model of a lamp post\n",
    "    based on its detailed description, using the google.genai SDK.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # The client is now passed as an argument to be reused.\n",
    "        prompt = f\"\"\"        You are an expert asset inspector. Your task is to find the **closest possible match** for the lamp post described below using the provided dataset.\n",
    "        Do not give up easily. It is crucial that you find a match if one exists, even if it's not perfect.\n",
    "\n",
    "        Description to analyze:\n",
    "        {description}\n",
    "\n",
    "        Compare this description against the \"Description/Key_Features\" and \"Dimensions\" fields in the dataset. Focus on specific details like \\\"250W High Pressure Sodium\\\" or dimensions like \\\"31.5\\\"D x 14.75\\\"W x 14\\\"H\\\".\n",
    "\n",
    "        Provide the answer in the format \"Make: [Make], Model: [Model]\".\n",
    "        **Only as a last resort**, if there is absolutely no resemblance to any entry, respond with \"Make: Unknown, Model: Unknown\".\n",
    "        \"\"\"\n",
    "\n",
    "        # Construct the tool configuration with the RAG corpus\n",
    "        tools = [\n",
    "            Tool(\n",
    "                retrieval=Retrieval(\n",
    "                    vertex_rag_store=VertexRagStore(\n",
    "                        rag_resources=[\n",
    "                            VertexRagStoreRagResource(\n",
    "                                rag_corpus=RAG_CORPUS_PATH\n",
    "                            )\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # As per your example, create a config object to hold the tools\n",
    "        generate_content_config = genai.types.GenerateContentConfig(\n",
    "            tools=tools\n",
    "        )\n",
    "        # Generate the content using the provided client and pass the tools via the config object\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            contents=[prompt],\n",
    "            generation_config=generate_content_config,\n",
    "        )\n",
    "\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error identifying make and model: {e}\")\n",
    "        return \"Make and model identification failed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8z9a1b2"
   },
   "source": [
    "## Process Images for Lamp Post Detection and Description\n",
    "\n",
    "This section iterates through a list of image URIs. For each image, it first checks if a lamp post is present using the `is_lamp_post` function. If a lamp post is detected, it then proceeds to generate a detailed description of the lamp post using the `get_detailed_description` function. Finally, it attempts to identify the make and model of the lamp post based on the detailed description using the `get_make_and_model` function, leveraging a RAG-grounded Gemini model. The results of these steps (whether a lamp post was detected, its description, and its identified make/model) are then printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3d4e5f6"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define Project and Region to ensure they are in scope\n",
    "PROJECT_ID = 'sarthaks-lab'\n",
    "REGION = 'us-central1'\n",
    "\n",
    "# Initialize the google.genai client once using the user-provided method\n",
    "final_results = []\n",
    "try:\n",
    "    # Per your instruction, initializing with project and location as direct arguments\n",
    "    genai_client = genai.Client(\n",
    "        vertexai=True, project=PROJECT_ID, location=REGION\n",
    "    )\n",
    "    print(\"google.genai client initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing google.genai client: {e}\")\n",
    "    genai_client = None\n",
    "\n",
    "# Main processing loop to analyze each image\n",
    "if genai_client:\n",
    "    for uri in gcs_uris:\n",
    "        print(f\"Processing image: {uri}\")\n",
    "\n",
    "        # Use the first function to check if it's a lamp post\n",
    "        if is_lamp_post(uri):\n",
    "            print(\"  -> Lamp post detected. Generating detailed description...\")\n",
    "\n",
    "            # If it is, get the detailed description\n",
    "            description = get_detailed_description(uri)\n",
    "            print(f\"  -> Generated Description: {description.strip()}\")\n",
    "\n",
    "            # Use the description to find the make and model with RAG\n",
    "            print(\"  -> Identifying make and model using RAG...\")\n",
    "            make_and_model = get_make_and_model(genai_client, description)\n",
    "            print(f\"  -> Identified Make and Model: {make_and_model.strip()}\")\n",
    "\n",
    "            final_results.append({\n",
    "                \"uri\": uri,\n",
    "                \"is_lamp_post\": True,\n",
    "                \"description\": description,\n",
    "                \"make_and_model\": make_and_model\n",
    "            })\n",
    "        else:\n",
    "            print(\"  -> Not a lamp post. Skipping analysis.\")\n",
    "            final_results.append({\n",
    "                \"uri\": uri,\n",
    "                \"is_lamp_post\": False,\n",
    "                \"description\": \"N/A\",\n",
    "                \"make_and_model\": \"N/A\"\n",
    "            })\n",
    "\n",
    "        print(\"-\"*50)\n",
    "\n",
    "    print(\"\\n--- Analysis Complete: Final Results ---\")\n",
    "    print(json.dumps(final_results, indent=2))\n",
    "else:\n",
    "    print(\"google.genai client was not initialized. Cannot proceed with analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7h8i9j1"
   },
   "source": [
    "## Process Images and Create DataFrame\n",
    "\n",
    "This section runs the analysis loop, stores the results, and then displays them in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k1l2m3n4"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Define Project and Region to ensure they are in scope\n",
    "PROJECT_ID = 'sarthaks-lab'  # @param {type:\"string\"}\n",
    "REGION = 'us-central1'      # @param {type:\"string\"}\n",
    "\n",
    "# This list will hold the structured data for the final DataFrame\n",
    "dataframe_results = []\n",
    "\n",
    "# Initialize the google.genai client\n",
    "try:\n",
    "    genai_client = genai.Client(\n",
    "        vertexai=True, project=PROJECT_ID, location=REGION\n",
    "    )\n",
    "    print(\"google.genai client initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing google.genai client: {e}\")\n",
    "    genai_client = None\n",
    "\n",
    "# Main processing loop\n",
    "if 'genai_client' in locals() and genai_client:\n",
    "    # Iterate through the full data fetched from BigQuery\n",
    "    for item in image_data_from_bq:\n",
    "        uri = item.get(\"gcs_uri\")\n",
    "        asset_id = item.get(\"asset_id\")\n",
    "        observation_id = item.get(\"observation_id\")\n",
    "\n",
    "        if not uri:\n",
    "            print(f\"Skipping item with missing gcs_uri: {item}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing image for asset {asset_id}: {uri}\")\n",
    "\n",
    "        if is_lamp_post(uri):\n",
    "            print(\"  -> Lamp post detected. Generating detailed description...\")\n",
    "            description = get_detailed_description(uri)\n",
    "\n",
    "            print(\"  -> Identifying make and model using RAG...\")\n",
    "            make_and_model_str = get_make_and_model(genai_client, description)\n",
    "            print(f\"  -> Identified: {make_and_model_str.strip()}\")\n",
    "\n",
    "            # Use regex to parse the Make and Model from the response string\n",
    "            match = re.match(r\"Make:\\s*(.*?),?\\s*Model:\\s*(.*)\", make_and_model_str.strip(), re.IGNORECASE)\n",
    "\n",
    "            if match:\n",
    "                make = match.group(1).strip()\n",
    "                model = match.group(2).strip()\n",
    "\n",
    "                # Add to results list only if a specific make/model was found\n",
    "                if make.lower() not in [\"unknown\", \"n/a\"] and model.lower() not in [\"unknown\", \"n/a\", \"generic incandescent fixture\", \"generic mv cobra head\"]:\n",
    "                    print(f\"  -> SUCCESS: Found Make: {make}, Model: {model}\")\n",
    "                    dataframe_results.append({\n",
    "                        \"asset_id\": asset_id,\n",
    "                        \"observation_id\": observation_id,\n",
    "                        \"make\": make,\n",
    "                        \"model\": model\n",
    "                    })\n",
    "                else:\n",
    "                    print(\"  -> INFO: Generic or unknown make/model found. Skipping for DataFrame.\")\n",
    "            else:\n",
    "                print(\"  -> INFO: Could not parse make and model from response. Skipping for DataFrame.\")\n",
    "        else:\n",
    "            print(\"  -> Not a lamp post. Skipping analysis.\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    print(\"\\n--- Analysis Complete --- \")\n",
    "    if dataframe_results:\n",
    "        results_df = pd.DataFrame(dataframe_results)\n",
    "        print(\"\\n--- Identified Lamp Posts --- \")\n",
    "        display(results_df)\n",
    "    else:\n",
    "        print(\"\\n--- No specific lamp post models were identified. ---\")\n",
    "else:\n",
    "    print(\"google.genai client was not initialized. Cannot proceed with analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5p6q7r8"
   },
   "source": [
    "## Save the dataframe to BigQuery\n",
    "\n",
    "This cell initializes the google.genai client, which is used for interacting with Gemini models.\n",
    "It takes the Google Cloud Project ID and Region as parameters.\n",
    "The client is initialized with vertexai=True to use Vertex AI's backend.\n",
    "Error handling is included in case the client initialization fails.\n",
    "The initialized client is stored in the 'genai_client' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s9t1u2v3"
   },
   "outputs": [],
   "source": [
    "def save_dataframe_to_bigquery(df, project_id, dataset_id, table_id):\n",
    "    \"\"\"\n",
    "    Saves a pandas DataFrame to a specified BigQuery table.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to save.\n",
    "        project_id (str): Your Google Cloud project ID.\n",
    "        dataset_id (str): The BigQuery dataset ID.\n",
    "        table_id (str): The BigQuery table ID to create or append to.\n",
    "    \"\"\"\n",
    "    if 'df' not in locals() or df.empty:\n",
    "        print(\"The DataFrame is empty or does not exist. Nothing to save to BigQuery.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "        print(f\"Attempting to save DataFrame to BigQuery table: {table_ref}\")\n",
    "        df.to_gbq(destination_table=table_ref, project_id=project_id, if_exists='append')\n",
    "        print(f\"Successfully saved DataFrame to BigQuery table: {table_ref}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving to BigQuery: {e}\")\n",
    "\n",
    "# --- Example Usage (edit and uncomment to run) ---\n",
    "# if 'results_df' in locals() and not results_df.empty:\n",
    "#     BIGQUERY_DATASET = 'your_dataset'      # @param {type:\"string\"}\n",
    "#     BIGQUERY_TABLE = 'identified_lamp_posts'   # @param {type:\"string\"}\n",
    "#     save_dataframe_to_bigquery(results_df, PROJECT_ID, BIGQUERY_DATASET, BIGQUERY_TABLE)\n",
    "# else:\n",
    "#     print(\"DataFrame 'results_df' not found or is empty. Skipping BigQuery upload.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
