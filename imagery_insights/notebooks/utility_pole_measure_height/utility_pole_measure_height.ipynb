{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "61d1bfb1",
      "metadata": {
        "id": "61d1bfb1"
      },
      "source": [
        "# Measure height of Utility poles with Gemini 2.5 Flash\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43939363",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "outputs": [],
      "source": [
        "## Description\n",
        "\n",
        "This notebook demonstrates how to estimate the height of utility poles using Google Cloud Vertex AI's Gemini 2.5 Flash model and imagery data stored in BigQuery and Google Cloud Storage. The workflow includes querying relevant image observations, grouping images by asset, and applying a multi-image AI-powered analysis to estimate asset heights with structured reasoning.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Access to a Google Cloud Platform (GCP) project with billing enabled.\n",
        "- BigQuery dataset containing utility pole imagery observations.\n",
        "- Google Cloud Storage bucket with image files referenced in BigQuery.\n",
        "- Vertex AI API enabled in your GCP project.\n",
        "- Service account or user credentials with permissions for BigQuery, Vertex AI, and Cloud Storage.\n",
        "- Python 3.8+ environment with internet access."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab074d90",
      "metadata": {
        "id": "ab074d90"
      },
      "source": [
        "## Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23f28877",
      "metadata": {
        "id": "23f28877"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade google-cloud-bigquery google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de710bd6",
      "metadata": {
        "id": "de710bd6"
      },
      "source": [
        "## Configuration\n",
        "\n",
        "**Important**: Replace the placeholder values below with your actual GCP Project ID and Region."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eefe0d49",
      "metadata": {
        "id": "eefe0d49"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = ''  # @param {type:\"string\"}\n",
        "REGION = 'us-central1'      # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64efff2f",
      "metadata": {
        "id": "64efff2f"
      },
      "source": [
        "## Imports and Vertex AI Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70c48cc9",
      "metadata": {
        "id": "70c48cc9"
      },
      "outputs": [],
      "source": [
        "import vertexai\n",
        "from google.cloud import bigquery\n",
        "from vertexai.generative_models import GenerativeModel, Part\n",
        "\n",
        "# Initialize Vertex AI SDK\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7df2c21e",
      "metadata": {
        "id": "7df2c21e"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = ''  # @param {type:\"string\"}\n",
        "REGION = 'us-central1'      # @param {type:\"string\"}\n",
        "\n",
        "from google.cloud import bigquery\n",
        "\n",
        "BIGQUERY_SQL_QUERY = \"\"\"\n",
        "\n",
        "SELECT\n",
        "  t1.gcs_uri,\n",
        "  t1.asset_id,\n",
        "  t1.observation_id,\n",
        "  t1.detection_time,\n",
        "  t1.location\n",
        "FROM\n",
        "  `sarthaks-lab`.`imagery_insights___preview___us`.`all_observations` AS t1\n",
        "WHERE\n",
        "  t1.asset_type = \"ASSET_CLASS_UTILITY_POLE\"\n",
        "  AND t1.asset_id IN (\n",
        "  SELECT\n",
        "    asset_id\n",
        "  FROM\n",
        "    `sarthaks-lab`.`imagery_insights___preview___us`.`all_observations`\n",
        "  WHERE\n",
        "    asset_type = \"ASSET_CLASS_UTILITY_POLE\"\n",
        "  GROUP BY\n",
        "    asset_id\n",
        "  HAVING\n",
        "    COUNT(observation_id) > 1\n",
        "  ORDER BY\n",
        "    asset_id  -- Add an ORDER BY for deterministic LIMIT behavior\n",
        "  LIMIT\n",
        "    10 );\n",
        "\"\"\"\n",
        "\n",
        "# Execute BigQuery Query\n",
        "try:\n",
        "    bigquery_client = bigquery.Client(project=PROJECT_ID)\n",
        "    query_job = bigquery_client.query(BIGQUERY_SQL_QUERY)\n",
        "    query_response_data = [dict(row) for row in query_job]\n",
        "\n",
        "    print(f\"Successfully fetched {len(query_response_data)} observations:\")\n",
        "    for item in query_response_data:\n",
        "        print(f\"Asset ID: {item['asset_id']}, GCS URI: {item['gcs_uri']}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while querying BigQuery: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e960933",
      "metadata": {
        "id": "1e960933"
      },
      "source": [
        "## Step 2: Group Images by Asset\n",
        "\n",
        "### Subtask: Group Images by Asset\n",
        "Add a new cell to process the query results and group the image GCS URIs by their corresponding `asset_id`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d9e4ae8",
      "metadata": {
        "id": "1d9e4ae8"
      },
      "source": [
        "**Reasoning**: Now that the data has been successfully queried from BigQuery, I will add a new cell to process the results. This cell will group the GCS URIs of the images by their `asset_id`, preparing the data for the next step where we will process each asset's images together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6b8afb7",
      "metadata": {
        "id": "a6b8afb7"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Group GCS URIs by asset_id\n",
        "assets = defaultdict(list)\n",
        "if 'query_response_data' in locals():\n",
        "    for item in query_response_data:\n",
        "        asset_id = item.get('asset_id')\n",
        "        gcs_uri = item.get('gcs_uri')\n",
        "        if asset_id and gcs_uri:\n",
        "            assets[asset_id].append(gcs_uri)\n",
        "\n",
        "    # Print the grouped assets\n",
        "    print(f\"Found {len(assets)} unique assets.\")\n",
        "    for asset_id, uris in assets.items():\n",
        "        print(f\"Asset ID: {asset_id}, Observations: {len(uris)}\")\n",
        "else:\n",
        "    print(\"No query response data found to process.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6aac5d3",
      "metadata": {
        "id": "d6aac5d3"
      },
      "source": [
        "## Step 3: Define Height Estimation Function\n",
        "\n",
        "### Subtask: Define Height Estimation Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c32a165",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c32a165",
        "outputId": "4849f7c2-6c38-4c8e-b08c-b5032e80987c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Height estimation function `estimate_asset_height` has been defined.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part\n",
        "\n",
        "def estimate_asset_height(gcs_uris: list[str]) -> dict:\n",
        "    \"\"\"\n",
        "    Estimates the height of an asset from a list of images using a Gemini Pro model.\n",
        "    \"\"\"\n",
        "    # Use a powerful model capable of analyzing multiple images and complex instructions.\n",
        "    MODEL = \"gemini-2.5-pro\"\n",
        "\n",
        "    # Initialize Vertex AI SDK if it hasn't been already\n",
        "    try:\n",
        "        vertexai.get_initialized_project()\n",
        "    except Exception:\n",
        "        vertexai.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "    prompt = \"\"\"\n",
        "{\n",
        "  \"task_description\": \"You will be provided with two or more images of the same utility asset. Your task is to estimate the height of the primary asset shown across all images as accurately and concisely as possible. Your reasoning must explicitly follow one of the provided reference analysis flows, leveraging geometrical principles, image understanding, and known reference values. Output must be in a structured JSON format suitable for database entry.\",\n",
        "  \"instructions\": [\n",
        "    {\n",
        "      \"step\": 1,\n",
        "      \"description\": \"Thoroughly analyze all provided images of the utility asset. Identify the primary utility asset and observe its placement within the environment in each image, noting any contextual clues.\"\n",
        "    },\n",
        "    {\n",
        "      \"step\": 2,\n",
        "      \"description\": \"Before applying specific flows, perform a general pre-estimation assessment for each image:\"\n",
        "    },\n",
        "    {\n",
        "      \"sub_step\": \"A. **Ground Plane Inference**: Identify the ground plane or base level of the asset. Note if it's flat, sloped, or obscured. If a horizon line is visible (especially in multiple images), infer camera tilt and relative elevation. If known parallel lines (e.g., road markings, building edges) converge, identify vanishing points to infer perspective. If the asset's base isn't ground level, determine its elevation from the ground.\",\n",
        "      \"mathematical_principles_applicable\": [\"Perspective Geometry\", \"Vanishing Point Analysis\"]\n",
        "    },\n",
        "    {\n",
        "      \"sub_step\": \"B. **Image Quality and Distortion Assessment**: Note any significant lens distortion (fish-eye, barrel/pincushion), blur, poor lighting, or occlusions that might affect measurement accuracy. These factors will directly influence your confidence score.\"\n",
        "    },\n",
        "    {\n",
        "      \"step\": 3,\n",
        "      \"description\": \"Scan all images for potential reference objects that have a generally known and consistent size. Prioritize references that are clearly visible and in useful proximity to the primary asset. Note their presence and estimated position relative to the ground plane in each image.\"\n",
        "    },\n",
        "    {\n",
        "      \"step\": 4,\n",
        "      \"description\": \"Apply the following reference analysis flows in order. Choose the first flow that matches the available references across all images. Proceed only with the chosen flow.\"\n",
        "    },\n",
        "    {\n",
        "      \"flow\": 1,\n",
        "      \"flow_id\": \"multi_image_correlated_reference\",\n",
        "      \"condition\": \"The *same specific reference object* (e.g., a uniquely identifiable vehicle, a particular person, a distinct piece of street furniture) is clearly identifiable and present in two or more images.\",\n",
        "      \"strategy\": \"Leverage the multiple views of this identical reference object to derive its most precise possible height, then use this refined reference height to estimate the asset's height. This method enhances accuracy by cross-validating the reference's dimensions. The underlying technique is often analogous to **Photogrammetric Triangulation or Multi-view Scale Calibration**, using known objects as 'control points' across views.\",\n",
        "      \"sub_steps\": [\n",
        "        \"A. **Derive Reference Object Height**: Mentally (or hypothetically, if pixel data were available) analyze the identified correlated reference object across all images where it appears. Use its perceived size relative to any other *stable reference* (e.g., a door height in a background image) or its dimensions from a clearer view to establish a more precise height for *this specific reference object*. Use perspective geometry inferred in Step 2.A to correct for camera angle effects on the reference's apparent height. If a shadow of the reference object is visible next to a shadow of the asset, consider **Shadow Triangulation** (see principles below) as an independent method to validate or refine asset height.\",\n",
        "        \"B. **Estimate Asset Height**: Using the newly derived (or confirmed) precise height of the correlated reference object, apply proportionality (distance-adjusted pixel height ratios) in one or more images where both the reference and asset are clear and at a similar depth. Account explicitly for perspective using inferred camera parameters from Step 2.A if objects are at different depths. If multiple images offer good views, use them to cross-validate or average the asset height estimates, incorporating potential uncertainty from each measurement.\"\n",
        "      ],\n",
        "      \"mathematical_principles_applicable\": [\n",
        "        {\"name\": \"Proportionality and Scaling\", \"description\": \"H_asset = H_ref * (P_asset / P_ref) where H is true height and P is pixel height, adjusted for perceived depth and camera angle/perspective. Robust H_ref from multiple views prior to this step improves accuracy.\"},\n",
        "        {\"name\": \"Trigonometry (Base-Height Relationship)\", \"description\": \"H = D * tan(angle_of_elevation_to_top), where D (horizontal distance) and angle can be more precisely inferred using the known reference object in multiple views relative to the inferred ground plane and camera perspective.\"},\n",
        "        {\"name\": \"Shadow Triangulation\", \"description\": \"If sun angle (altitude and azimuth) is known or can be estimated (e.g., from time/date/location), H = Shadow_Length / tan(Sun_Altitude_Angle). If a reference object's height and its shadow are known, and the asset's shadow is known, asset_height = ref_height * (asset_shadow_length / ref_shadow_length). Leverage this method if shadows are clear and conditions allow.\"}\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"flow\": 2,\n",
        "      \"flow_id\": \"multiple_distinct_references\",\n",
        "      \"condition\": \"The same specific reference object (as in Flow 1) is NOT identifiable across multiple images, BUT two or more *different* reference objects (or references in only one image) are identifiable across the dataset (e.g., a car in Image 1 and a person in Image 2, or both a car and a person in Image 1).\",\n",
        "      \"strategy\": \"Utilize each available reference object independently to estimate the asset's height. Then, combine these individual estimates for a more robust final height. The underlying technique is often **Weighted Averaging or Consensus Aggregation** of multiple independent measurements, potentially with explicit consideration for measurement uncertainty.\",\n",
        "      \"sub_steps\": [\n",
        "        \"A. **Individual Asset Height Estimates**: For each identifiable reference object across all images, use its known (or assumed average) height to estimate the asset's height in the specific image(s) where it appears clearly and is in useful proximity. Account explicitly for perspective using inferred camera parameters from Step 2.A if objects are at different depths. Record each individual estimate along with its perceived trustworthiness (e.g., based on reference clarity, proximity, and ground plane visibility). Consider **Shadow Triangulation** if feasible for any reference/asset pair.\",\n",
        "        \"B. **Combine Estimates**: Aggregate all individual asset height estimates. Prioritize estimates derived from references that were clearer, closer to the asset, and less affected by perspective distortion (reflecting an implicit or explicit 'weighting'). If estimates vary significantly, assess the sources of discrepancy based on Step 2.B (image quality/distortion) and the specific reference characteristics.\"\n",
        "      ],\n",
        "      \"mathematical_principles_applicable\": [\n",
        "        {\"name\": \"Proportionality and Scaling\", \"description\": \"H_asset = H_ref * (P_asset / P_ref), adjusted for perspective.\"},\n",
        "        {\"name\": \"Shadow Triangulation\", \"description\": \"H = Shadow_Length / tan(Sun_Altitude_Angle), if conditions are met.\"}\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"flow\": 3,\n",
        "      \"flow_id\": \"single_reference_single_image\",\n",
        "      \"condition\": \"Only one single primary reference object can be clearly identified in only one of the provided images, or references are too unclear/distant in other images to be useful. OR, no suitable reference objects are found, but *contextual asset standards* are strong.\",\n",
        "      \"strategy\": \"Estimate the asset's height based solely on this single available reference, or if no references are truly suitable, use general knowledge of typical asset heights. Acknowledge the inherently lower confidence due to limited cross-validation and fewer data points. The underlying technique is **Direct Proportional Scaling from a Single Reference** or **Contextual Heuristic Estimation**.\",\n",
        "      \"sub_steps\": [\n",
        "        \"A. **Estimate Asset Height**: Use the known (or assumed average) height of the single identifiable reference object to estimate the asset's height. Account explicitly for perspective using inferred camera parameters from Step 2.A. If **Shadow Triangulation** is feasible, attempt it. If no suitable references are found at all, estimate based on **Contextual Asset Standards** (e.g., 'this looks like a standard distribution pole, which are typically 10-12m').\",\n",
        "        \"B. **Confidence Adjustment**: Reflect the lower confidence due to the solitary nature of the reference or reliance on heuristic knowledge.\"\n",
        "      ],\n",
        "      \"mathematical_principles_applicable\": [\n",
        "        {\"name\": \"Proportionality and Scaling\", \"description\": \"H_asset = H_ref * (P_asset / P_ref), adjusted for perspective.\"},\n",
        "        {\"name\": \"Shadow Triangulation\", \"description\": \"H = Shadow_Length / tan(Sun_Altitude_Angle), if possible.\"},\n",
        "        {\"name\": \"Contextual Heuristics\", \"description\": \"Leveraging typical dimensions for known asset types when direct measurement references are insufficient.\"}\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"step\": 5,\n",
        "      \"description\": \"Provide your final findings exclusively in the following JSON format. Ensure no extraneous text, notes, or markdown formatting appear outside of the JSON block itself. Your 'reasoning_notes' must be concise and structured.\"\n",
        "    }\n",
        "  ],\n",
        "  \"reference_objects_guidelines\": {\n",
        "    \"priority\": [\n",
        "      \"Utility Poles (e.g., common wooden utility poles often have standardized heights, typically ranging from 10-15 meters depending on voltage lines and context)\",\n",
        "      \"Vehicles (e.g., passenger cars are approximately 1.4-1.8 meters high, pickup trucks are approximately 1.8-2.1 meters high, semi-trailer trucks have varying heights but the trailer body is often around 4.1 meters)\",\n",
        "      \"Human Figures (e.g., an average adult height is 1.6-1.8 meters; note if it appears to be a child or a significantly taller/shorter adult)\",\n",
        "      \"Doors (e.g., standard residential doors are typically 2.0-2.1 meters high, commercial doors can be larger)\",\n",
        "      \"Windows (e.g., a standard window pane height can vary, but common residential windows might be 1.2-1.5 meters tall)\",\n",
        "      \"Building Levels (e.g., a single-story residential building is typically 3-4 meters high; a commercial floor can be 3.5-5 meters high)\",\n",
        "      \"Manhole Covers (e.g., diameter typically around 0.6-0.75 meters)\",\n",
        "      \"Fire Hydrants (e.g., height varies but generally around 0.6-1 meter)\",\n",
        "      \"Street Furniture (e.g., benches, trash cans, traffic signs - provide specific types and estimated sizes if possible)\",\n",
        "      \"Consistent Ground Level Features (e.g., curb height, sidewalk thickness, road markings)\"\n",
        "    ],\n",
        "    \"notes\": \"When using a reference object, be as specific as possible about its type and estimated dimensions. If its size is variable, break down the asset's height into proportional segments. When combining images, analyze how the reference object appears relative to the asset in different views. Crucially, if visual cues suggest a measurable geometric relationship (e.g., distinct angles, proportional pixel heights), describe the relevant mathematical principle and the values you would hypothetically input to perform the calculation. **Explicitly comment on how ground plane inference, perspective, and potential image distortions were considered.**\"\n",
        "  },\n",
        "  \"output_format\": {\n",
        "    \"json_structure\": {\n",
        "      \"estimated_height_meters\": \"<estimated_height_as_a_float>\",\n",
        "      \"confidence_score\": \"<Low/Medium/High>\",\n",
        "      \"modeling_technique_applied\": \"<Name of the modeling technique used based on the flow_id, e.g., 'Multi-Image Scale Calibration', 'Weighted Averaging', 'Direct Proportional Scaling'>\",\n",
        "      \"reasoning_notes\": {\n",
        "        \"flow_id_followed\": \"<ID of the strategy flow chosen from 'instructions.step 4'>\",\n",
        "        \"pre_estimation_assessment\": {\n",
        "          \"ground_plane_inference\": \"<Brief note on ground plane (e.g., 'flat, clearly visible in img1/2', 'sloped, inferred from road markings', 'obscured, assumed from asset base'). Mention horizon/vanishing points if used.>\",\n",
        "          \"image_quality_issues\": \"<Concise notes on any significant distortions, blur, or occlusions identified (e.g., 'minor barrel distortion img1', 'asset part obscured img2', 'clear throughout').>\"\n",
        "        },\n",
        "        \"primary_references_details\": \"[{\\\"object_type\\\": \\\"<type_of_object>\\\", \\\"identifier\\\": \\\"<specific_id_if_correlated_e.g._'blue_sedan'>\\\", \\\"images_present\\\": [\\\"<img_id_1>\\\", \\\"<img_id_2>\\\"], \\\"assumed_or_derived_height_m\\\": <float_height>, \\\"derivation_notes\\\": \\\"<brief_justification_for_height_e.g._'standard_avg'/'derived_from_imgA_compared_to_door'/'cross-validated_across_img1/2'>\\\"}]\",\n",
        "        \"secondary_references_details\": \"[{\\\"object_type\\\": \\\"<type_of_object>\\\", \\\"images_present\\\": [\\\"<img_id_1>\\\"], \\\"assumed_or_derived_height_m\\\": <float_height>, \\\"derivation_notes\\\": \\\"<brief_justification_for_height>\\\"}]\",\n",
        "        \"calculation_summary\": \"<Concise summary of how the final height was derived, e.g., 'Proportional scaling from correlated blue sedan (1.65m) in img1/2, adjusted for inferred perspective' or 'Averaged estimates from Car (1.5m) in img1 and Person (1.7m) in img2, favoring car due to proximity.'>\",\n",
        "        \"mathematical_principles_applied\": [\"<Principle1>\", \"<Principle2>\"],\n",
        "        \"confidence_factors\": \"[{'factor': 'Reference Clarity', 'impact': 'positive', 'details': 'All references very sharp and close'}, {'factor': 'Perspective Distortion', 'impact': 'minor_negative', 'details': 'Slight camera tilt in img2, corrected'}, {'factor': 'Shadow Availability', 'impact': 'positive', 'details': 'Sun angle allowed triangulation in img1'}]\",\n",
        "        \"confidence_justification\": \"<Brief overall statement, e.g., 'High due to multiple clear, consistent references across views with perspective correction and shadow validation.'>\"\n",
        "      }\n",
        "    },\n",
        "    \"fallback_if_no_estimation\": {\n",
        "      \"estimated_height_meters\": null,\n",
        "      \"confidence_score\": \"Low\",\n",
        "      \"modeling_technique_applied\": \"No Estimation\",\n",
        "      \"reasoning_notes\": {\n",
        "        \"flow_id_followed\": \"no_estimation\",\n",
        "        \"pre_estimation_assessment\": {\n",
        "          \"ground_plane_inference\": \"Unable to clearly identify.\",\n",
        "          \"image_quality_issues\": \"Severe blur and occlusion.\"\n",
        "        },\n",
        "        \"primary_references_details\": [],\n",
        "        \"secondary_references_details\": [],\n",
        "        \"calculation_summary\": \"Unable to make a reasonable estimation due to insufficient clear reference objects, inconsistent data across images, poor image quality, or inability to identify applicable mathematical relationships or perform multi-image correlation.\",\n",
        "        \"mathematical_principles_applied\": [],\n",
        "        \"confidence_factors\": [{\"factor\": \"Reference Availability\", \"impact\": \"negative\", \"details\": \"No reliable references present.\"}],\n",
        "        \"confidence_justification\": \"Lack of reliable data and poor image quality.\"\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        model = GenerativeModel(MODEL)\n",
        "\n",
        "        image_parts = [Part.from_uri(uri=uri, mime_type=\"image/jpeg\") for uri in gcs_uris]\n",
        "\n",
        "        content = [prompt] + image_parts\n",
        "\n",
        "        responses = model.generate_content(content)\n",
        "\n",
        "        response_text = responses.text.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
        "        result = json.loads(response_text)\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error estimating height for URIs {gcs_uris}: {e}\")\n",
        "        return {\n",
        "            \"estimated_height_meters\": None,\n",
        "            \"confidence_score\": \"Error\",\n",
        "            \"reasoning_notes\": str(e)\n",
        "        }\n",
        "\n",
        "print(\"Height estimation function `estimate_asset_height` has been defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07da8280",
      "metadata": {
        "id": "07da8280"
      },
      "source": [
        "## Step 4: Process Assets and Generate DataFrame\n",
        "\n",
        "### Subtask: Process Assets and Generate DataFrame\n",
        "Iterate through the grouped assets, call the new height estimation function for each, and compile the results (asset ID, number of observations, measured height, and confidence score) into a pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95e368a7",
      "metadata": {
        "id": "95e368a7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "from IPython.display import display\n",
        "\n",
        "# Suppress the specific deprecation warning from the Vertex AI SDK\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"This feature is deprecated as of June 24, 2025\")\n",
        "\n",
        "# List to store the results\n",
        "results_data = []\n",
        "\n",
        "print(\"--- Processing Assets and Estimating Height ---\\n\")\n",
        "\n",
        "if 'assets' in locals() and assets:\n",
        "    # Iterate through each asset and its associated image URIs\n",
        "    for asset_id, uris in assets.items():\n",
        "        # Call the height estimation function\n",
        "        estimation_result = estimate_asset_height(uris)\n",
        "\n",
        "        # Extract results for formatted printing\n",
        "        height = estimation_result.get(\"estimated_height_meters\", \"N/A\")\n",
        "        confidence = estimation_result.get(\"confidence_score\", \"N/A\")\n",
        "\n",
        "        # Pretty print the immediate result as requested\n",
        "        print(f\"Asset ID: {asset_id}\\n  - Height: {height} meters\\n  - Confidence: {confidence}\\n\")\n",
        "\n",
        "        # Append the full results to our list for the DataFrame\n",
        "        results_data.append({\n",
        "            \"asset_id\": asset_id,\n",
        "            \"num_observations\": len(uris),\n",
        "            \"estimated_height_meters\": height,\n",
        "            \"confidence_score\": confidence,\n",
        "            \"reasoning_notes\": estimation_result.get(\"reasoning_notes\", \"N/A\")\n",
        "        })\n",
        "\n",
        "    # Create a pandas DataFrame from the results\n",
        "    results_df = pd.DataFrame(results_data)\n",
        "\n",
        "    # Display the final DataFrame in a clean, 'pretty' format\n",
        "    print(\"\\n--- Final Results Summary ---\")\n",
        "    display(results_df)\n",
        "\n",
        "else:\n",
        "    print(\"No assets found to process.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "name": "utility_pole_measure_height.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
